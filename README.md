# Segment Anything Model + PEFT

## Prompting in Segmentation
- The prompt can be a number of things, ranging from __input points__, __bounding boxes__, a __rough drawing__, or even a __text prompt__.

![SAM-Arch](./assets/image.png)

- There is an encoder which will embed the given image into a latet dimension. The authors choice for the encoder was a Vision Transformer (VIT) pretrained on a __masked image modelling__ task.
- To encode the prompt:
    - For __Text__, the CLIP Embeddings are used.
    - For __Dense__ features or other masks, they use a __Conv__ model.
    - For __Points__ or __Boxes__, they use __Positional Encoding__.
- Then the image and the prompt embeddings are fused together with a element wise summation.
- Then, this is passed into a Decoder to get back the original masks.
    - For the decoder, a __modified transformer decoder__ was used.
- To train the model, a linear combination of __Focal Loss__ and __Dice Loss__.
- The output are multiple to eliminate ambiguity.

## Training Process
The training procedure is different from a normal neural network, since this is a Foundation Model.
- A custom data engine was used which had a total 1.1 Billion masks over 11 Million Images.
- This data engine was developed in 3 stages:
    1. __Assisted Manual Stage__:
        - They trained the SAM on commonly available public datasets and let this model interact with manual annotators, who modified the output mask to make it as accurate as possible.
        - Now, after the corrected data was achieved, they re-train the model on these new masks which was generated by the annotators.
            - This cycle of periodic retraining was done 6 times.
        - During these 6 times, the encoder size was also increased from ViT-B to a larger ViT-H model.
        > At the end of these 6 stages, there was a total of 120K images with 4.3M masks, and the output mask per image was around 44.
    2. __Semi-Automatic Stage__:
        - This stage focusses on improving the diversity of the images.
        - The annotators we asked to annotate the additional unlabeled objects that were much more detailed, thus took longer at this stage.
        - By, the end of this stage, they annotated 180K images with over 5.9M masks, with an approximate of 72 masks per image.
        - There yet another round of periodic retraining, but it was done only 5 times.
    3. __Fully-automatic Stage__:
        - Introduced prompting at the input, with __32x32 regular grids__ as input and the output would a __part, sub-part and the whole object__.
        - To further refine the quality, they introduce zoomed image crops to improve further.
        - At the end of this stage, they had 11M images and 1.1B masks leading to the __SA-1B dataset__.

### SA-1B Dataset
- Even though it has 1B masks, these masks are automatically generated by the SAM model.
- They have really high resolution images (3300x4950) downsampled to 1500px on short side.

## Zero-Shot Transfer Learning

> __Main Goal__: get SAM work on tasks never trained for (produce valid mask from any promt).

- __Tasks__:
    1. __Single Point Valid Mask Generation__
        - We input one point and the model produces the mask corresponding to the object (knowledge from the part, sub-part and entire object).
    2. __Low-level Vision: Edge Detection__
        - Given an image, we expect the model to identify the edges in the image.
    3. __Mid-level Vision: Object Proposals__
        - In many object detection systems, these proposals are then evaluated by the object detection model to see which proposal is best suited for the proposal.
    3. __High-Level Vision: Instance Segmentation__
        - We initially pass the image into another model for getting the object proposals using something like ViTDiT or something similar and this results in the object proposal phase are then sent in as prompts to the SAM model and this will result in instance segmentation outputs.
